# ğŸ“˜ Handwritten Digit Classification â€“ SVM vs Random Forest

**Machine Learning Project | MNIST Dataset**

---

## ğŸ“Œ 1. Project Overview

This project evaluates and compares two classical machine learning modelsâ€”**Support Vector Machine (SVM)** and **Random Forest (RF)**â€”for handwritten digit classification using the **MNIST dataset**.
The goal is to study the impact of preprocessing, analyze the classification performance of both models, and determine which algorithm is better suited for digit recognition under typical machine learning constraints.

---

## ğŸ¯ 2. Motivation

Digit recognition is a foundational step in many automation systems. Accurate handwritten digit classification improves efficiency in:

* Banking systems (cheque reading, form digitization)
* Postal services (automated sorting of handwritten addresses)
* Document processing and OCR systems
* Automated data entry pipelines

The MNIST dataset provides a benchmark platform to compare models and understand their suitability for real-world digit recognition tasks.

---

## ğŸ“‚ 3. Repository Structure

```
â”œâ”€â”€ data/
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_loader.py
â”‚   â”œâ”€â”€ preprocess.py
â”‚   â”œâ”€â”€ train_svm.py
â”‚   â”œâ”€â”€ train_rf.py
â”‚   â””â”€â”€ evaluate.py
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_EDA.ipynb
â”‚   â”œâ”€â”€ 02_SVM_Model.ipynb
â”‚   â””â”€â”€ 03_RF_Model.ipynb
â”‚
â”œâ”€â”€ figures/
â”‚   â”œâ”€â”€ class_distribution.png
â”‚   â”œâ”€â”€ svm_confusion_matrix.png
â”‚   â”œâ”€â”€ rf_confusion_matrix.png
â”‚   â””â”€â”€ sample_digits.png
â”‚
â”œâ”€â”€ reports/
â”‚   â””â”€â”€ MNIST_Final_Report.pdf
â”‚
â”œâ”€â”€ slides/
â”‚   â””â”€â”€ MNIST_Presentation.pptx
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ run.sh
â””â”€â”€ README.md
```

---

## ğŸ“Š 4. Dataset Description

**Dataset:** MNIST â€“ Handwritten Digit Database
**Source:** OpenML (554) / Yann LeCun
**Format:** Grayscale images, 28Ã—28 pixels
**Total Samples:** 70,000

* 60,000 training
* 10,000 testing

**Classes:** Digits from **0 to 9** (10 classes)
**Distribution:** Balanced across all digits
**Feature Extraction:** Each 28Ã—28 image is flattened into a **784-feature vector**

---

## ğŸ” 5. Exploratory Data Analysis (EDA)

Key findings from dataset inspection:

* **Balanced class distribution**, enabling unbiased model training
* **Distinct pixel intensity patterns** for each digit
* **Natural handwriting variations** introduce realistic classification difficulty
* **Clean and noise-free data**, ideal for controlled ML experiments
* Visual sample inspection confirms diverse writing styles and shape distortions

EDA included visualizations such as:

* Class distribution bar chart
* Sample digit grids
* Pixel intensity histograms
* Confusion matrices for both models

---

## âš™ï¸ 6. Preprocessing Pipeline

1. **Load MNIST** via OpenML
2. **Flatten** 28Ã—28 images into 784-dimensional vectors
3. Convert pixel intensity to `float32`
4. **Normalize** intensity values to the range **0â€“1** using MinMaxScaler
5. Shuffle dataset for randomness
6. Use default MNIST train/test split
7. Scale data specifically for SVM (mandatory due to kernel sensitivity)
8. Use same scaled data for RF for consistency

This preprocessing ensures both models receive standardized input vectors.

---

## ğŸ¤– 7. Models Implemented

### **Support Vector Machine (SVM)**

* Kernel: **RBF**
* Hyperparameters tuned:

  * `C` in [1, 5, 10]
  * `gamma` in [0.001, 0.01]
* Strong performance on high-dimensional data
* Sensitive to feature scaling
* More computationally expensive

### **Random Forest (RF)**

* 200 decision trees
* Max depth: 15â€“40
* Bootstrap sampling enabled
* Robust to noise and overfitting
* Faster training time
* Works well without heavy preprocessing

---

## ğŸ“ˆ 8. Results & Discussion

Both models achieved high accuracy, demonstrating MNISTâ€™s suitability for classical ML algorithms.

### **Performance Summary**

| Model             | Accuracy | Training Time | Key Notes                                   |
| ----------------- | -------- | ------------- | ------------------------------------------- |
| **SVM (RBF)**     | ~96-97%   | Slower        | Best accuracy, strong at complex boundaries |
| **Random Forest** | ~96â€“98%  | Faster        | More robust, simpler, faster to train       |

### **Insights**

* SVM excelled in capturing fine pixel-level differences
* RF is more scalable and less sensitive to hyperparameters
* RF misclassifies visually similar digits more often (3â†”5, 4â†”9)

### **Conclusion**

* **Random Forest outperformed SVM overall in this experiment**, considering the combination of accuracy and training speed
* The better model depends on:

  * Dataset size
  * Accuracy requirements
  * Execution time constraints
  * Infrastructure limitations

---

## ğŸ§ª 9. Reproducibility

To reproduce the project:

### **Install Dependencies**

```bash
pip install -r requirements.txt
```

### **Run Entire Pipeline**

```bash
bash run.sh
```

### **Manual Execution**

```bash
python src/data_loader.py
python src/preprocess.py
python src/train_svm.py
python src/train_rf.py
python src/evaluate.py
```

All scripts follow deterministic operations to ensure repeatable results.

---

## ğŸ§‘â€ğŸ’» 10. Individual Contribution

This project includes the following contributions:

* Complete preprocessing pipeline
* SVM and Random Forest model implementation
* EDA (datasets, charts, sample visualizations)
* Hyperparameter exploration
* Evaluation and comparison
* Final report creation
* Presentation slides
* Version control and repository documentation

---

## ğŸ”® 11. Future Work

* Implement **CNN-based digit classification** (Deep Learning)
* Use **PCA** for dimensionality reduction
* Perform **grid search / random search** for improved hyperparameters
* Add **data augmentation** to simulate real-world samples
* Explore **gradient boosting models**

---

## ğŸ“ 12. References

* Yann LeCun, â€œThe MNIST Databaseâ€
* Scikit-Learn Documentation
* OpenML MNIST Dataset
* Cortes & Vapnik â€“ SVM Foundations
* Breiman â€“ Random Forests

---

